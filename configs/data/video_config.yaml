# Video Data Configuration for Temporal Localization
# ===================================================

# Dataset settings
dataset:
  name: "video_temporal_grounding"
  data_root: "./data"
  annotation_file: "./data/annotations/train_2k5_train.json"
  video_dir: "./data/videos"

  # Data format specification
  # Supports two annotation formats:
  # 1. JSON array format: A single JSON array containing annotation objects
  # 2. JSONL format: One JSON object per line (legacy format)
  #
  # Each annotation object should contain:
  #   video: str - video file path (e.g., "./data/videos/example.mp4")
  #   duration: float - video duration in seconds
  #   timestamp: [float, float] - start and end time of target segment
  #   sentence: str - text query describing the moment
  #   video_start: float (optional) - trim video from this time
  #   video_end: float (optional) - trim video to this time
  #   qid: str (optional) - unique query identifier
  #   difficulty: float (optional) - sample difficulty score
  #   pred: [float, float] (optional) - model predictions for evaluation
  #
  # Note: Additional metadata fields can be added directly to each JSON object.
  # These fields will be collected automatically as metadata.

# Video processing settings
video:
  # Maximum number of frames to sample
  max_frames: 32
  # # Frame sampling strategy: uniform, random, keyframe
  # sampling_strategy: "uniform"
  # # Image resolution for model input
  # resolution:
  #   height: 384
  #   width: 384
  # Frame rate for processing (0 = use original)
  fps: 2
  # # Video normalization
  # normalize:
  #   mean: [0.485, 0.456, 0.406]
  #   std: [0.229, 0.224, 0.225]

  # Video frame resolution limits (Qwen2-VL recommended settings)
  # These parameters help control GPU memory usage by limiting video frame resolution.
  # According to Qwen2-VL official guidance:
  # - Set min_pixels and max_pixels to limit individual frame resolution
  # - Set total_pixels to limit total tokens across all frames
  #   (recommended: less than 24576 * 32 * 32 to avoid excessively long input sequences)
  #
  # Minimum pixels per frame (default: 4 * 32 * 32 = 4096)
  min_pixels: 4096
  # Maximum pixels per frame (default: 8 * 32 * 32 = 8192)
  max_pixels: 8192
  # Total pixels across all frames (default: 20480 * 32 * 32 = 20971520)
  # Set to null to disable total pixel limit
  total_pixels: 20971520

# # Text processing settings
# text:
#   # Maximum query length (tokens)
#   max_query_length: 77
#   # Tokenizer name (will be loaded from model)
#   tokenizer: null

# Temporal settings
temporal:
  # Number of temporal bins for discretization
  num_bins: 1000
  # Whether to use relative timestamps (0-1) or absolute (not work when using temporal tokens)
  use_relative_timestamps: true
  # Whether to use temporal tokens (<0>~<999>) instead of float values
  # When enabled, adds new tokens to vocabulary (not replacing existing tokens)
  use_temporal_tokens: true
  # Embedding initialization strategy for temporal tokens:
  # - "sinusoidal": Use sinusoidal positional encoding (recommended for faster convergence)
  # - "mean": Use mean of existing embeddings
  # - "random": Random initialization
  embedding_init_strategy: "sinusoidal"
  # Whether to only calculate loss on temporal response tokens
  # When enabled, only <|box_start|>, temporal tokens (e.g., <250>), and <|box_end|>
  # will contribute to the loss calculation. All other tokens (video, prompt, etc.)
  # will be masked from loss computation.
  temporal_loss_only: true

  # # Output format: "tokens" for text generation, "class" for classification
  # output_format: "tokens"
  # # Temporal output tokens (for token replacement)
  # special_tokens:
  #   start_token: "<time_start>"
  #   end_token: "<time_end>"
  #   separator_token: "<time_sep>"

# # Data loading settings
# dataloader:
#   batch_size: 4
#   num_workers: 4
#   prefetch_factor: 2
#   pin_memory: true
#   shuffle: true
#   drop_last: true

# Duration-based batch sampling configuration
# When enabled, videos are grouped into batches based on total duration
# instead of a fixed batch size. This helps stabilize GPU memory usage
# when videos have varying lengths (since frame count = duration * fps).
duration_batching:
  # Enable duration-based batch sampling
  enabled: true
  # Target total duration (in seconds) for each batch
  # Example: If target_batch_duration=60 and avg video is 30s, expect ~2 videos/batch
  target_batch_duration: 800.0
  # Maximum number of samples per batch (optional constraint)
  # Set to null to disable this constraint
  max_batch_size: null
  # Minimum number of samples per batch
  min_batch_size: 1
  # Whether to drop the last incomplete batch
  drop_last: false

# # Data augmentation (optional, for training)
# augmentation:
#   enabled: false
#   # Temporal jittering range (fraction of duration)
#   temporal_jitter: 0.05
#   # Color jittering
#   color_jitter:
#     brightness: 0.1
#     contrast: 0.1
#     saturation: 0.1
#     hue: 0.05

# Validation settings
validation:
  annotation_file: "./data/annotations/train_2k5_val.json"
  # batch_size: 8
  # shuffle: false
