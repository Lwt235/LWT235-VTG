# Reinforcement Learning (GRPO/R1) Configuration
# ================================================

# Model configuration
model:
  # Path to SFT checkpoint or base model
  name_or_path: "./outputs/sft/checkpoint-best"
  # Alternative: start from base model
  # name_or_path: "Qwen/Qwen3-VL-4B-Instruct"
  
  trust_remote_code: true
  torch_dtype: "bfloat16"
  attn_implementation: "flash_attention_2"

# Reference model (for KL divergence)
ref_model:
  # Use same as model or separate checkpoint
  name_or_path: null  # null means clone from model
  load_in_8bit: false

# LoRA configuration (if continuing LoRA training)
lora:
  enabled: true
  r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"
  # Modules to fully train (not LoRA) - required for temporal tokens
  modules_to_save:
    - "embed_tokens"
    - "lm_head"

# GRPO-specific configuration
grpo:
  # Number of generations per prompt
  num_generations: 4
  
  # Generation settings
  generation:
    max_new_tokens: 64
    temperature: 0.7
    top_p: 0.9
    top_k: 50
    do_sample: true
    num_beams: 1
  
  # Advantage estimation
  advantage:
    # Baseline type: "mean", "moving_average", "none"
    baseline_type: "mean"
    # Normalize advantages
    normalize: true
    # Clip advantages
    clip_range: 10.0
  
  # Policy optimization
  policy:
    # PPO-style clipping
    clip_range: 0.2
    # KL divergence coefficient
    kl_coef: 0.05
    # Entropy bonus coefficient
    entropy_coef: 0.01

# Training configuration
training:
  output_dir: "./outputs/rl"
  
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 16
  
  num_train_epochs: 1
  max_steps: 1000
  
  learning_rate: 5.0e-6
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  
  optim: "adamw_torch"
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  bf16: true
  fp16: false
  gradient_checkpointing: true
  
  logging_dir: "./outputs/rl/logs"
  logging_steps: 10
  report_to: "tensorboard"
  
  eval_strategy: "steps"
  eval_steps: 100
  
  save_strategy: "steps"
  save_steps: 100
  save_total_limit: 3
  
  seed: 42
  dataloader_num_workers: 4

# Reward configuration
rewards:
  # Reward weights (normalized internally)
  temporal_iou:
    enabled: true
    weight: 1.0
  segment_overlap:
    enabled: true
    weight: 0.5
  step_consistency:
    enabled: false  # Reserved for future
    weight: 0.0
  
  # Reward processing
  processing:
    # Normalize rewards per batch
    normalize: true
    # Clip reward values
    clip_min: -10.0
    clip_max: 10.0
    # Scale factor for final reward
    scale: 1.0

# Rollout configuration
rollout:
  # Number of rollout steps before update
  rollout_length: 128
  # Buffer size for experience replay
  buffer_size: 512
  # Number of epochs per rollout
  epochs_per_rollout: 4

# Data configuration path
data:
  config_path: "./configs/data/video_config.yaml"

# DeepSpeed configuration path
deepspeed: null
